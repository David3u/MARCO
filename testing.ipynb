{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from task import Task\n",
    "import trainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, tasks):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on a set of tasks with explicit shape prediction metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        tasks: List of Task objects\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics including node-level accuracy and shape prediction metrics\n",
    "    \"\"\"\n",
    "    total_tasks = len(tasks)\n",
    "    total_train_grids = sum(len(task.train_pairs) for task in tasks)\n",
    "    total_test_grids = sum(len(task.test_pairs) for task in tasks)\n",
    "    total_grids = total_train_grids + total_test_grids\n",
    "    \n",
    "    correct_tasks = 0\n",
    "    correct_train_grids = 0\n",
    "    correct_test_grids = 0\n",
    "    \n",
    "    # Node-level accuracy tracking\n",
    "    total_train_nodes = 0\n",
    "    total_test_nodes = 0\n",
    "    correct_train_nodes = 0\n",
    "    correct_test_nodes = 0\n",
    "    \n",
    "    # Shape prediction metrics\n",
    "    total_shape_predictions = 0\n",
    "    correct_shape_predictions = 0\n",
    "    \n",
    "    # Evaluate each task\n",
    "    with tqdm(total=len(tasks), desc=\"Evaluating model on tasks\") as pbar:\n",
    "        for task in tasks:\n",
    "            # Run model to get predictions\n",
    "            predictions = model.solve(task)\n",
    "            \n",
    "            # Check predictions match\n",
    "            if len(predictions) != len(task.test_pairs):\n",
    "                print(f\"Warning: Number of predictions ({len(predictions)}) doesn't match number of test pairs ({len(task.test_pairs)}) for task {task.task_id}\")\n",
    "            \n",
    "            # Count correct train grids and nodes\n",
    "            train_correct = 0\n",
    "\n",
    "            train_predictions = model.solve(task)\n",
    "            \n",
    "            for i, ((_, expected), predicted) in enumerate(zip(task.train_pairs, train_predictions)):\n",
    "                expected_np = np.array(expected)\n",
    "                \n",
    "                # First check if shape is correct\n",
    "                shape_is_correct = (predicted.shape == expected_np.shape)\n",
    "                \n",
    "                # For grid-level accuracy: if shape is wrong, it's automatically incorrect\n",
    "                if not shape_is_correct:\n",
    "                    is_correct = False\n",
    "                else:\n",
    "                    # Shapes match, now check content\n",
    "                    is_correct = np.array_equal(predicted, expected_np)\n",
    "                \n",
    "                # For node-level metrics, still calculate based on the overlapping area\n",
    "                # This gives us some measure of content accuracy even with wrong shape\n",
    "                if not shape_is_correct:\n",
    "                    # If shape doesn't match, resize for node comparison only\n",
    "                    resized_prediction = np.zeros(expected_np.shape, dtype=predicted.dtype)\n",
    "                    min_rows = min(predicted.shape[0], expected_np.shape[0])\n",
    "                    min_cols = min(predicted.shape[1], expected_np.shape[1])\n",
    "                    resized_prediction[:min_rows, :min_cols] = predicted[:min_rows, :min_cols]\n",
    "                    predicted_for_node_check = resized_prediction\n",
    "                else:\n",
    "                    predicted_for_node_check = predicted\n",
    "                \n",
    "                # Calculate node-level accuracy\n",
    "                train_grid_total_nodes = expected_np.size\n",
    "                train_grid_correct_nodes = np.sum(predicted_for_node_check == expected_np)\n",
    "                \n",
    "                total_train_nodes += train_grid_total_nodes\n",
    "                correct_train_nodes += train_grid_correct_nodes\n",
    "                    \n",
    "                # Update grid level metrics\n",
    "                if is_correct:\n",
    "                    train_correct += 1\n",
    "                    correct_train_grids += 1\n",
    "            \n",
    "            # Count correct test grids and nodes\n",
    "            test_correct = 0\n",
    "            \n",
    "            for i, ((_, expected), predicted) in enumerate(zip(task.test_pairs, predictions)):\n",
    "                expected_np = np.array(expected)\n",
    "                \n",
    "                # Track shape prediction accuracy\n",
    "                total_shape_predictions += 1\n",
    "                shape_is_correct = (predicted.shape == expected_np.shape)\n",
    "                if shape_is_correct:\n",
    "                    correct_shape_predictions += 1\n",
    "                    \n",
    "                # For grid-level accuracy: if shape is wrong, it's automatically incorrect\n",
    "                if not shape_is_correct:\n",
    "                    is_correct = False\n",
    "                else:\n",
    "                    # Shapes match, now check content\n",
    "                    is_correct = np.array_equal(predicted, expected_np)\n",
    "                \n",
    "                # For node-level metrics, still calculate based on the overlapping area\n",
    "                if not shape_is_correct:\n",
    "                    # If shape doesn't match, resize for node comparison only\n",
    "                    resized_prediction = np.zeros(expected_np.shape, dtype=predicted.dtype)\n",
    "                    min_rows = min(predicted.shape[0], expected_np.shape[0])\n",
    "                    min_cols = min(predicted.shape[1], expected_np.shape[1])\n",
    "                    resized_prediction[:min_rows, :min_cols] = predicted[:min_rows, :min_cols]\n",
    "                    predicted_for_node_check = resized_prediction\n",
    "                else:\n",
    "                    predicted_for_node_check = predicted\n",
    "                \n",
    "                # Calculate node-level accuracy\n",
    "                test_grid_total_nodes = expected_np.size\n",
    "                test_grid_correct_nodes = np.sum(predicted_for_node_check == expected_np)\n",
    "                \n",
    "                total_test_nodes += test_grid_total_nodes\n",
    "                correct_test_nodes += test_grid_correct_nodes\n",
    "                    \n",
    "                # Update grid level metrics\n",
    "                if is_correct:\n",
    "                    test_correct += 1\n",
    "                    correct_test_grids += 1\n",
    "            \n",
    "            # A task is correct if all its test pairs are correct\n",
    "            if test_correct == len(task.test_pairs):\n",
    "                correct_tasks += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Calculate overall grid metrics\n",
    "    correct_grids = correct_train_grids + correct_test_grids\n",
    "    train_grid_accuracy = correct_train_grids / total_train_grids if total_train_grids > 0 else 0\n",
    "    test_grid_accuracy = correct_test_grids / total_test_grids if total_test_grids > 0 else 0\n",
    "    overall_grid_accuracy = correct_grids / total_grids if total_grids > 0 else 0\n",
    "    task_accuracy = correct_tasks / total_tasks if total_tasks > 0 else 0\n",
    "    \n",
    "    # Calculate shape prediction metrics\n",
    "    shape_accuracy = correct_shape_predictions / total_shape_predictions if total_shape_predictions > 0 else 0\n",
    "    \n",
    "    # Calculate node-level metrics\n",
    "    total_nodes = total_train_nodes + total_test_nodes\n",
    "    correct_nodes = correct_train_nodes + correct_test_nodes\n",
    "    train_node_accuracy = correct_train_nodes / total_train_nodes if total_train_nodes > 0 else 0\n",
    "    test_node_accuracy = correct_test_nodes / total_test_nodes if total_test_nodes > 0 else 0\n",
    "    overall_node_accuracy = correct_nodes / total_nodes if total_nodes > 0 else 0\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Correct tasks: {correct_tasks}/{total_tasks} ({task_accuracy:.2%})\")\n",
    "    print(f\"Correct test grids: {correct_test_grids}/{total_test_grids} ({test_grid_accuracy:.2%})\")\n",
    "    print(f\"Shape prediction accuracy: {correct_shape_predictions}/{total_shape_predictions} ({shape_accuracy:.2%})\")\n",
    "    print(f\"Correct train grids: {correct_train_grids}/{total_train_grids} ({train_grid_accuracy:.2%})\")\n",
    "    print(f\"Correct total grids: {correct_grids}/{total_grids} ({overall_grid_accuracy:.2%})\")\n",
    "    print(f\"\\nNode-level Accuracy:\")\n",
    "    print(f\"Test nodes: {correct_test_nodes}/{total_test_nodes} ({test_node_accuracy:.2%})\")\n",
    "    print(f\"Train nodes: {correct_train_nodes}/{total_train_nodes} ({train_node_accuracy:.2%})\")\n",
    "    print(f\"Overall nodes: {correct_nodes}/{total_nodes} ({overall_node_accuracy:.2%})\")\n",
    "    \n",
    "    return {\n",
    "        \"task_accuracy\": task_accuracy,\n",
    "        \"test_grid_accuracy\": test_grid_accuracy,\n",
    "        \"shape_accuracy\": shape_accuracy,\n",
    "        \"train_grid_accuracy\": train_grid_accuracy,\n",
    "        \"overall_grid_accuracy\": overall_grid_accuracy,\n",
    "        \"correct_tasks\": correct_tasks,\n",
    "        \"total_tasks\": total_tasks,\n",
    "        \"correct_test_grids\": correct_test_grids,\n",
    "        \"total_test_grids\": total_test_grids,\n",
    "        \"correct_shape_predictions\": correct_shape_predictions,\n",
    "        \"total_shape_predictions\": total_shape_predictions,\n",
    "        \"correct_train_grids\": correct_train_grids,\n",
    "        \"total_train_grids\": total_train_grids,\n",
    "        \"correct_grids\": correct_grids,\n",
    "        \"total_grids\": total_grids,\n",
    "        # Node-level metrics\n",
    "        \"test_node_accuracy\": test_node_accuracy,\n",
    "        \"train_node_accuracy\": train_node_accuracy,\n",
    "        \"overall_node_accuracy\": overall_node_accuracy,\n",
    "        \"correct_test_nodes\": correct_test_nodes,\n",
    "        \"total_test_nodes\": total_test_nodes,\n",
    "        \"correct_train_nodes\": correct_train_nodes,\n",
    "        \"total_train_nodes\": total_train_nodes,\n",
    "        \"correct_nodes\": correct_nodes,\n",
    "        \"total_nodes\": total_nodes\n",
    "    }\n",
    "\n",
    "def preprocess_task_graphs(tasks, padding_value=10):\n",
    "    \"\"\"\n",
    "    Preprocess all graphs in the given tasks once, instead of during each solve call.\n",
    "    \n",
    "    Args:\n",
    "        tasks: List of Task objects\n",
    "        padding_value: Padding value for standardizing dimensions\n",
    "        \n",
    "    Returns:\n",
    "        The tasks with preprocessed graphs\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    expected_dim = 3  # Standardized input dimension\n",
    "    \n",
    "    for task in tasks:\n",
    "        # Preprocess test graphs\n",
    "        for test_graph in task.test_graphs:\n",
    "            if hasattr(test_graph, 'x'):\n",
    "                # Convert from one-hot to class labels if needed\n",
    "                if test_graph.x.dim() == 2 and test_graph.x.size(1) == 11:\n",
    "                    test_graph.x = test_graph.x.argmax(dim=1)\n",
    "\n",
    "                # Ensure x is long and 2D\n",
    "                test_graph.x = test_graph.x.long()\n",
    "                if test_graph.x.dim() == 1:\n",
    "                    test_graph.x = test_graph.x.unsqueeze(1)  # Shape: (nodes, 1)\n",
    "\n",
    "                # Standardize shape to (nodes, expected_dim)\n",
    "                if test_graph.x.size(1) < expected_dim:\n",
    "                    pad = torch.full((test_graph.x.size(0), expected_dim), padding_value, dtype=torch.long)\n",
    "                    pad[:, :test_graph.x.size(1)] = test_graph.x\n",
    "                    test_graph.x = pad\n",
    "                elif test_graph.x.size(1) > expected_dim:\n",
    "                    test_graph.x = test_graph.x[:, :expected_dim]\n",
    "\n",
    "                # Extract positional info\n",
    "                test_graph.pos = test_graph.x[:, 1:3].float() if expected_dim >= 3 else None\n",
    "                test_graph.x = test_graph.x[:, 0].long().unsqueeze(1)  # Final x: shape (nodes, 1)\n",
    "                \n",
    "                # Mark as preprocessed\n",
    "                test_graph.preprocessed = True\n",
    "        \n",
    "        # If the task also has train graphs, preprocess them too\n",
    "        if hasattr(task, 'train_graphs'):\n",
    "            for train_graph in task.train_graphs:\n",
    "                if hasattr(train_graph, 'x'):\n",
    "                    # Convert from one-hot to class labels if needed\n",
    "                    if train_graph.x.dim() == 2 and train_graph.x.size(1) == 11:\n",
    "                        train_graph.x = train_graph.x.argmax(dim=1)\n",
    "    \n",
    "                    # Ensure x is long and 2D\n",
    "                    train_graph.x = train_graph.x.long()\n",
    "                    if train_graph.x.dim() == 1:\n",
    "                        train_graph.x = train_graph.x.unsqueeze(1)  # Shape: (nodes, 1)\n",
    "    \n",
    "                    # Standardize shape to (nodes, expected_dim)\n",
    "                    if train_graph.x.size(1) < expected_dim:\n",
    "                        pad = torch.full((train_graph.x.size(0), expected_dim), padding_value, dtype=torch.long)\n",
    "                        pad[:, :train_graph.x.size(1)] = train_graph.x\n",
    "                        train_graph.x = pad\n",
    "                    elif train_graph.x.size(1) > expected_dim:\n",
    "                        train_graph.x = train_graph.x[:, :expected_dim]\n",
    "    \n",
    "                    # Extract positional info\n",
    "                    train_graph.pos = train_graph.x[:, 1:3].float() if expected_dim >= 3 else None\n",
    "                    train_graph.x = train_graph.x[:, 0].long().unsqueeze(1)  # Final x: shape (nodes, 1)\n",
    "                    \n",
    "                    # Mark as preprocessed\n",
    "                    train_graph.preprocessed = True\n",
    "    \n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading precomputed tasks from precomputed_tasks/evaluation_400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading tasks from precomputed_tasks/evaluation_400: 100%|██████████| 400/400 [00:02<00:00, 144.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 precomputed tasks\n",
      "\n",
      "Starting evaluation run 1/5\n",
      "Initializing nlm model...\n",
      "Model state loaded from output/models/nlm_shape/nlm_shape_final.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks:   0%|          | 2/400 [00:01<05:39,  1.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3640855/845477698.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mresults_all_runs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3640855/902765508.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, tasks)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# Run model to get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# Check predictions match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis-files/nlm_shape.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                     \u001b[0;31m# Apply edge transformation predictions if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict_edge_transformations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_edge_transformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m                     \u001b[0;31m# Convert model output back to grid with predicted shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis-files/nlm_shape.py\u001b[0m in \u001b[0;36mpredict_edge_transformations\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    817\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mpred_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Assuming type 1 means \"remove edge\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                         \u001b[0medges_to_remove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                     \u001b[0;32melif\u001b[0m \u001b[0mpred_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Assuming type 2 means \"add edge\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m                         \u001b[0;31m# For adding edges, we need the edge type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m                         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from unified_shape import UnifiedReasoningModule\n",
    "from nlm_shape import NLMReasoningModule\n",
    "import trainer\n",
    "from task5 import Blackboard\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model path\n",
    "UNIFIED_PATH = \"output/models/unified_shape/unified_shape_final.pt\"\n",
    "NLM_PATH = \"output/models/nlm_shape/nlm_shape_final.pt\"\n",
    "DATA_PATH = \"precomputed_tasks/evaluation_400\"\n",
    "\n",
    "results_all_runs = []\n",
    "\n",
    "# Load tasks\n",
    "tasks = trainer.load_precomputed_tasks(DATA_PATH)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nStarting evaluation run {i+1}/5\")\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Reset task states if needed\n",
    "    for task in tasks:\n",
    "        if hasattr(task, 'blackboard'):\n",
    "            task.blackboard = Blackboard()\n",
    "    \n",
    "    # Initialize model\n",
    "    # print(\"Initializing unified model...\")\n",
    "    # model = UnifiedReasoningModule(\n",
    "    #     input_dim=3,\n",
    "    #     hidden_dim=128,\n",
    "    #     output_dim=11,\n",
    "    #     device=device\n",
    "    # )\n",
    "    # model.load_complete_state(UNIFIED_PATH)\n",
    "\n",
    "    \n",
    "    # Initialize model and preprocess tasks for nlm\n",
    "    trainer.preprocess_task_graphs(tasks)\n",
    "    print(\"Initializing nlm model...\")\n",
    "    model = NLMReasoningModule(\n",
    "            input_dim=3,\n",
    "            hidden_dim=128,\n",
    "            output_dim=11,\n",
    "            device=device\n",
    "        )\n",
    "    model.load_complete_state(NLM_PATH)\n",
    "\n",
    "    # Move model to device\n",
    "    model.model = model.model.to(device)\n",
    "    model.model.eval() # Set to evaluation mode\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluate_model(model, tasks)\n",
    "    results_all_runs.append(results)\n",
    "    \n",
    "    # Delete model to ensure clean state for next iteration\n",
    "    del model\n",
    "    \n",
    "    # Clear CUDA cache again\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Analyze results across all runs\n",
    "print(\"\\nResults across all runs:\")\n",
    "for i, run_results in enumerate(results_all_runs):\n",
    "    print(f\"Run {i+1}: {run_results}\")\n",
    "\n",
    "# Calculate average performance\n",
    "if results_all_runs:\n",
    "    avg_accuracy = sum(r.get('accuracy', 0) for r in results_all_runs) / len(results_all_runs)\n",
    "    print(f\"Average accuracy: {avg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading evaluation tasks...\n",
      "Loading precomputed tasks from precomputed_tasks/evaluation_400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading tasks from precomputed_tasks/evaluation_400: 100%|██████████| 400/400 [00:02<00:00, 148.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 precomputed tasks\n",
      "\n",
      "=== Starting evaluation run 1/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:10...\n",
      "Loaded MAML model from output/models/unified_shape/unified_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [06:05<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 1 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 2/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:10...\n",
      "Loaded MAML model from output/models/unified_shape/unified_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [06:07<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 2 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 3/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:10...\n",
      "Loaded MAML model from output/models/unified_shape/unified_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [06:06<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 3 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 4/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:10...\n",
      "Loaded MAML model from output/models/unified_shape/unified_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [06:06<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 4 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 5/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:10...\n",
      "Loaded MAML model from output/models/unified_shape/unified_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [06:05<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 5 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "Average metrics across 5 runs:\n",
      "Full accuracy: 0.0000 (shape + content)\n",
      "Shape accuracy: 0.6611 (correct dimensions)\n",
      "Content accuracy: 0.5306 (overlapping cells)\n",
      "Task accuracy: 0.0000\n",
      "Grid accuracy: 0.0000\n",
      "Node accuracy: 0.5306\n",
      "Evaluation completed and results saved.\n",
      "Loading evaluation tasks...\n",
      "Loading precomputed tasks from precomputed_tasks/evaluation_400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading tasks from precomputed_tasks/evaluation_400: 100%|██████████| 400/400 [00:02<00:00, 147.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 precomputed tasks\n",
      "\n",
      "=== Starting evaluation run 1/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:15...\n",
      "Loaded MAML model from output/models/unified_shape/unified_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [09:07<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 1 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 2/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:15...\n",
      "Loaded MAML model from output/models/unified_shape/unified_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [09:09<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 2 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 3/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:15...\n",
      "Loaded MAML model from output/models/unified_shape/unified_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [09:07<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 3 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 4/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:15...\n",
      "Loaded MAML model from output/models/unified_shape/unified_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [09:07<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 4 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 5/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:15...\n",
      "Loaded MAML model from output/models/unified_shape/unified_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [09:08<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 5 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "Average metrics across 5 runs:\n",
      "Full accuracy: 0.0000 (shape + content)\n",
      "Shape accuracy: 0.6611 (correct dimensions)\n",
      "Content accuracy: 0.5306 (overlapping cells)\n",
      "Task accuracy: 0.0000\n",
      "Grid accuracy: 0.0000\n",
      "Node accuracy: 0.5306\n",
      "Evaluation completed and results saved.\n",
      "Loading evaluation tasks...\n",
      "Loading precomputed tasks from precomputed_tasks/evaluation_400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading tasks from precomputed_tasks/evaluation_400: 100%|██████████| 400/400 [00:02<00:00, 154.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 precomputed tasks\n",
      "\n",
      "=== Starting evaluation run 1/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:10...\n",
      "Loaded MAML model from output/models/nlm_shape/nlm_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [24:04<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 1 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 2/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:10...\n",
      "Loaded MAML model from output/models/nlm_shape/nlm_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [24:05<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 2 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 3/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:10...\n",
      "Loaded MAML model from output/models/nlm_shape/nlm_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [23:59<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 3 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 4/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:10...\n",
      "Loaded MAML model from output/models/nlm_shape/nlm_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [23:39<00:00,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 4 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 5/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:10...\n",
      "Loaded MAML model from output/models/nlm_shape/nlm_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [23:55<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 5 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "Average metrics across 5 runs:\n",
      "Full accuracy: 0.0000 (shape + content)\n",
      "Shape accuracy: 0.6611 (correct dimensions)\n",
      "Content accuracy: 0.5306 (overlapping cells)\n",
      "Task accuracy: 0.0000\n",
      "Grid accuracy: 0.0000\n",
      "Node accuracy: 0.5306\n",
      "Evaluation completed and results saved.\n",
      "Loading evaluation tasks...\n",
      "Loading precomputed tasks from precomputed_tasks/evaluation_400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading tasks from precomputed_tasks/evaluation_400: 100%|██████████| 400/400 [00:02<00:00, 155.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 precomputed tasks\n",
      "\n",
      "=== Starting evaluation run 1/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:15...\n",
      "Loaded MAML model from output/models/nlm_shape/nlm_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [26:35<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 1 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 2/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:15...\n",
      "Loaded MAML model from output/models/nlm_shape/nlm_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [26:37<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 2 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 3/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:15...\n",
      "Loaded MAML model from output/models/nlm_shape/nlm_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [26:39<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 3 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 4/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:15...\n",
      "Loaded MAML model from output/models/nlm_shape/nlm_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [26:39<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 4 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "=== Starting evaluation run 5/5 ===\n",
      "Evaluating with MAML task adaptation with ILR:0.05 and IS:15...\n",
      "Loaded MAML model from output/models/nlm_shape/nlm_maml_final_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model on tasks: 100%|██████████| 400/400 [26:41<00:00,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Correct tasks: 0/400 (0.00%)\n",
      "Correct test grids: 0/419 (0.00%)\n",
      "Shape prediction accuracy: 277/419 (66.11%)\n",
      "Correct train grids: 0/1363 (0.00%)\n",
      "Correct total grids: 0/1782 (0.00%)\n",
      "\n",
      "Node-level Accuracy:\n",
      "Test nodes: 52273/98515 (53.06%)\n",
      "Train nodes: 44892/80688 (55.64%)\n",
      "Overall nodes: 97165/179203 (54.22%)\n",
      "Run 5 metrics:\n",
      "  Full accuracy: 0.0000 (shape + content)\n",
      "  Shape accuracy: 0.6611 (correct dimensions)\n",
      "  Content accuracy: 0.5306 (overlapping cells)\n",
      "  Grid accuracy: 0.0000\n",
      "  Node accuracy: 0.5306\n",
      "\n",
      "Average metrics across 5 runs:\n",
      "Full accuracy: 0.0000 (shape + content)\n",
      "Shape accuracy: 0.6611 (correct dimensions)\n",
      "Content accuracy: 0.5306 (overlapping cells)\n",
      "Task accuracy: 0.0000\n",
      "Grid accuracy: 0.0000\n",
      "Node accuracy: 0.5306\n",
      "Evaluation completed and results saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from unified_module import UnifiedReasoningModule\n",
    "from nlm_module import NLMReasoningModule\n",
    "from task_adaptation_runner import TaskAdaptationRunner\n",
    "import trainer\n",
    "from task5 import Blackboard, Task\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model paths\n",
    "UNIFIED_MAML_PATH = \"output/models/unified_shape/unified_maml_final_model.pt\"\n",
    "NLM_MAML_PATH = \"output/models/nlm_shape/nlm_maml_final_model.pt\"\n",
    "USE_BLACKBOARD_INSIGHTS = True  # Whether to use insights from blackboard\n",
    "\n",
    "def evaluate_task_adaptation(tasks, model_path, reasoning_module_class, method=\"maml\", inner_lr=0.001, inner_steps=5):\n",
    "    \"\"\"\n",
    "    Evaluate using task adaptation with the TaskAdaptationRunner\n",
    "    \n",
    "    Args:\n",
    "        tasks: List of tasks to evaluate\n",
    "        model_path: Path to the pre-trained meta-learning model\n",
    "        reasoning_module_class: Class of reasoning module (UnifiedReasoningModule or NLMReasoningModule)\n",
    "        method: Meta-learning method (\"maml\" or \"proto\")\n",
    "        inner_lr: Inner loop learning rate\n",
    "        inner_steps: Number of inner loop steps\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation results including shape metrics\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating with {method.upper()} task adaptation with ILR:{inner_lr} and IS:{inner_steps}...\")\n",
    "    \n",
    "    # Determine reasoning method string\n",
    "    reasoning_method = \"unified\" if reasoning_module_class.__name__ == \"UnifiedReasoningModule\" else \"nlm\"\n",
    "    \n",
    "    # Initialize task adaptation runner with the correct device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    runner = TaskAdaptationRunner(\n",
    "        inner_lr=inner_lr,\n",
    "        inner_steps=inner_steps,\n",
    "        model_path=model_path,\n",
    "        model_config={'input_dim': 3},\n",
    "        reasoning_module_class=reasoning_module_class,\n",
    "        method=method,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create a wrapper class to make TaskAdaptationRunner compatible with evaluate_model\n",
    "    class TaskAdaptationWrapper:\n",
    "        def __init__(self, runner):\n",
    "            self.runner = runner\n",
    "            \n",
    "        def solve(self, task):\n",
    "            return self.runner.adapt_to_task(\n",
    "                task=task,\n",
    "                visualize=False,\n",
    "                save_dir=None\n",
    "            )\n",
    "    \n",
    "    # Create the wrapper instance\n",
    "    model_wrapper = TaskAdaptationWrapper(runner)\n",
    "    \n",
    "    # Get full results from evaluate_model\n",
    "    results = evaluate_model(model_wrapper, tasks)\n",
    "    \n",
    "    # Return the metrics without redundant recalculation\n",
    "    return {\n",
    "        \"full_accuracy\": results['test_grid_accuracy'],\n",
    "        \"shape_accuracy\": results['shape_accuracy'],\n",
    "        \"content_accuracy\": results['test_node_accuracy'],\n",
    "        \"task_accuracy\": results['task_accuracy'],\n",
    "        \"test_grid_accuracy\": results['test_grid_accuracy'],\n",
    "        \"test_node_accuracy\": results['test_node_accuracy']\n",
    "    }\n",
    "\n",
    "def run_evaluation(tasks, reasoning_module_class, model_path, inner_lr, inner_steps):\n",
    "    \"\"\"Run multiple evaluation runs and return averaged results with shape metrics\"\"\"\n",
    "    num_runs = 5  # Number of evaluation runs\n",
    "    all_results = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\n=== Starting evaluation run {run+1}/{num_runs} ===\")\n",
    "        \n",
    "        # Clear CUDA cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Reset tasks' blackboards if needed\n",
    "        for task in tasks:\n",
    "            if hasattr(task, 'blackboard'):\n",
    "                task.blackboard = Blackboard()\n",
    "        \n",
    "        # Run evaluation with task adaptation\n",
    "        run_results = evaluate_task_adaptation(\n",
    "            tasks=tasks,\n",
    "            model_path=model_path,\n",
    "            reasoning_module_class=reasoning_module_class,\n",
    "            inner_lr=inner_lr,\n",
    "            inner_steps=inner_steps,\n",
    "            method=\"maml\"\n",
    "        )\n",
    "        \n",
    "        all_results.append(run_results)\n",
    "        \n",
    "        # Print comprehensive metrics for this run\n",
    "        print(f\"Run {run+1} metrics:\")\n",
    "        print(f\"  Full accuracy: {run_results.get('full_accuracy', 0.0):.4f} (shape + content)\")\n",
    "        print(f\"  Shape accuracy: {run_results.get('shape_accuracy', 0.0):.4f} (correct dimensions)\")\n",
    "        print(f\"  Content accuracy: {run_results.get('content_accuracy', 0.0):.4f} (overlapping cells)\")\n",
    "        print(f\"  Grid accuracy: {run_results.get('test_grid_accuracy', 0.0):.4f}\")\n",
    "        print(f\"  Node accuracy: {run_results.get('test_node_accuracy', 0.0):.4f}\")\n",
    "    \n",
    "    # Calculate average performance for all metrics\n",
    "    metrics = [\n",
    "        \"full_accuracy\", \"shape_accuracy\", \"content_accuracy\", \n",
    "        \"test_grid_accuracy\", \"test_node_accuracy\", \"task_accuracy\"\n",
    "    ]\n",
    "    \n",
    "    avg_metrics = {}\n",
    "    for metric in metrics:\n",
    "        # Use get() to handle cases where old metrics might be missing\n",
    "        values = [result.get(metric, 0.0) for result in all_results]\n",
    "        avg_metrics[f\"average_{metric}\"] = sum(values) / len(values) if values else 0.0\n",
    "    \n",
    "    # Print comprehensive average metrics\n",
    "    print(f\"\\nAverage metrics across {num_runs} runs:\")\n",
    "    print(f\"Full accuracy: {avg_metrics.get('average_full_accuracy', 0.0):.4f} (shape + content)\")\n",
    "    print(f\"Shape accuracy: {avg_metrics.get('average_shape_accuracy', 0.0):.4f} (correct dimensions)\")\n",
    "    print(f\"Content accuracy: {avg_metrics.get('average_content_accuracy', 0.0):.4f} (overlapping cells)\")\n",
    "    print(f\"Task accuracy: {avg_metrics.get('average_task_accuracy', 0.0):.4f}\")\n",
    "    print(f\"Grid accuracy: {avg_metrics.get('average_test_grid_accuracy', 0.0):.4f}\")\n",
    "    print(f\"Node accuracy: {avg_metrics.get('average_test_node_accuracy', 0.0):.4f}\")\n",
    "    \n",
    "    # Return comprehensive results\n",
    "    return {\n",
    "        \"all_runs\": all_results,\n",
    "        **avg_metrics  # Include all average metrics\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MODELS = [\"unified\", \"nlm\"]\n",
    "    INNER_LR = 0.05\n",
    "    INNER_STEPS = [5, 10, 15]\n",
    "    \n",
    "    for MODEL in MODELS:\n",
    "        for IS in INNER_STEPS:\n",
    "            \n",
    "            # Load evaluation tasks\n",
    "            print(\"Loading evaluation tasks...\")\n",
    "            tasks = trainer.load_precomputed_tasks(\"precomputed_tasks/evaluation_400\")\n",
    "            \n",
    "            # Run evaluation\n",
    "            if MODEL==\"nlm\":\n",
    "                tasks = trainer.preprocess_task_graphs(tasks)\n",
    "                results = run_evaluation(tasks, NLMReasoningModule, NLM_MAML_PATH, INNER_LR, IS)\n",
    "            else:\n",
    "                results = run_evaluation(tasks, UnifiedReasoningModule, UNIFIED_MAML_PATH, INNER_LR, IS)\n",
    "            \n",
    "            # Convert numpy values to Python native types for JSON serialization\n",
    "            def convert_numpy(obj):\n",
    "                if isinstance(obj, np.number):\n",
    "                    return float(obj)\n",
    "                elif isinstance(obj, np.ndarray):\n",
    "                    return obj.tolist()\n",
    "                elif isinstance(obj, dict):\n",
    "                    return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "                elif isinstance(obj, list):\n",
    "                    return [convert_numpy(i) for i in obj]\n",
    "                return obj\n",
    "            \n",
    "            with open(f\"{MODEL}_{IS}_05_task_adaptation_results.json\", \"w\") as f:\n",
    "                json.dump(convert_numpy(results), f, indent=2)\n",
    "            \n",
    "            print(\"Evaluation completed and results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LLM Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading up to 3 tasks...\n",
      "Loaded 3 tasks\n",
      "Creating LLM module with model: ft:gpt-4o-mini-2024-07-18:personal:arc-agi-blackboard:BJoraD1a\n",
      "Loaded 7 cached responses\n",
      "Evaluating LLM module...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e319003c3484685830fdd4f00284b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ft:gpt-4o-mini-2024-07-18:personal:arc-agi-blackboard:BJoraD1a:   0%|          | 0/3 [00:00<?, ?it/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "Task 1/3: a85d4709.json\n",
      "================================================================================\n",
      "\n",
      "Initial blackboard state:\n",
      "  Blackboard summary:\n",
      "    - Reasoning steps: 0\n",
      "    - Knowledge items: 1\n",
      "    - Has transformations: False\n",
      "\n",
      "Solving with LLM module...\n",
      "Analyzing grid transformation pattern.\n",
      "\n",
      "BLACKBOARD_OUTPUT:\n",
      "```json\n",
      "```\n",
      "{\"confidence\": 0.6, \"logical_predicates\": {\"transform_from_0_to_3\": [\"0\", \"2\", \"3\", \"4\", \"6\", \"8\"], \"transform_from_5_to_3\": [\"1\", \"5\", \"7\"], \"transform_from_0_to_4\": [\"0\", \"2\", \"3\", \"4\", \"6\", \"8\"], \"transform_from_5_to_4\": [\"1\", \"5\", \"7\"], \"transform_from_5_to_2\": [\"0\", \"4\", \"6\"]}, \"transformations\": [], \"predictions\": [{\"test_index\": 0, \"input_grid\": [[0, 0, 5], [5, 0, 0], [0, 5, 0]], \"output_grid\": [[3, 3, 3], [2, 2, 2], [4, 4, 4]], \"changes\": {\"total_changes\": 9, \"percentage_changed\": 100.0, \"changes_by_color\": {\"0->3\": 2, \"5->3\": 1, \"5->2\": 1, \"0->2\": 2, \"0->4\": 2, \"5->4\": 1}}}]}\n",
      "```\n",
      "\n",
      "PREDICTED OUTPUTS:\n",
      "Test Example 1 Input:\n",
      "0 0 5\n",
      "5 0 0\n",
      "0 5 0\n",
      "\n",
      "Test Example 1 Output:\n",
      "3 3 3\n",
      "2 2 2\n",
      "4 4 4\n",
      "\n",
      "\n",
      "Error parsing blackboard data: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Blackboard state after LLM reasoning:\n",
      "  Blackboard summary:\n",
      "    - Reasoning steps: 1\n",
      "    - Knowledge items: 3\n",
      "    - Has transformations: False\n",
      "\n",
      "Test example 1:\n",
      "  - Exact match: False\n",
      "  - Cell accuracy: 0.0000\n",
      "\n",
      "Task summary:\n",
      "  - Exact matches: 0/1\n",
      "  - Task accuracy: 0.0000\n",
      "  - Execution time: 0.00 seconds\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Task 2/3: c8cbb738.json\n",
      "================================================================================\n",
      "\n",
      "Initial blackboard state:\n",
      "  Blackboard summary:\n",
      "    - Reasoning steps: 0\n",
      "    - Knowledge items: 1\n",
      "    - Has transformations: False\n",
      "\n",
      "Solving with LLM module...\n",
      "Analyzing grid transformation pattern.\n",
      "\n",
      "BLACKBOARD_OUTPUT:\n",
      "```json\n",
      "```\n",
      "{\"confidence\": 0.9, \"logical_predicates\": {\"grid_size_change\": true, \"input_grid_rows\": 12, \"input_grid_cols\": 11, \"output_grid_rows\": 5, \"output_grid_cols\": 5}, \"transformations\": [], \"predictions\": [{\"test_index\": 0, \"input_grid\": [[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 1, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 2, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 2, 8], [8, 8, 8, 3, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 8, 6, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 3, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 8, 6, 8, 8], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]], \"output_grid\": [[3, 8, 6, 1, 6, 8, 3], [8, 8, 8, 8, 8, 8, 8], [6, 8, 8, 8, 8, 8, 6], [3, 8, 6, 1, 6, 8, 3], [8, 8, 8, 8, 8, 8, 8], [6, 8, 8, 8, 8, 8, 6], [3, 8, 6, 1, 6, 8, 3]], \"changes\": {\"total_changes\": \"N/A (different shapes)\", \"percentage_changed\": 0, \"changes_by_color\": {}}}]}\n",
      "```\n",
      "\n",
      "PREDICTED OUTPUTS:\n",
      "Test Example 1 Input:\n",
      "8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      "8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      "8 8 8 8 8 8 1 8 8 8 8 8 8 8 8 8 8 8 8\n",
      "8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      "8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      "8 8 8 1 8 8 8 8 8 1 8 8 8 8 8 8 8 8 8\n",
      "8 8 8 8 8 8 8 8 8 8 8 2 8 8 8 8 8 2 8\n",
      "8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      "8 8 8 8 8 8 1 8 8 8 8 2 8 8 8 8 8 2 8\n",
      "8 8 8 3 8 8 8 8 8 3 8 8 8 8 8 8 8 8 8\n",
      "8 8 8 8 8 8 8 8 8 8 8 8 8 8 6 8 6 8 8\n",
      "8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      "8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      "8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      "8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      "8 8 8 3 8 8 8 8 8 3 8 8 8 8 8 8 8 8 8\n",
      "8 8 8 8 8 8 8 8 8 8 8 8 8 8 6 8 6 8 8\n",
      "8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      "\n",
      "Test Example 1 Output:\n",
      "3 8 6 1 6 8 3\n",
      "8 8 8 8 8 8 8\n",
      "6 8 8 8 8 8 6\n",
      "3 8 6 1\n",
      "Error parsing blackboard data: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Blackboard state after LLM reasoning:\n",
      "  Blackboard summary:\n",
      "    - Reasoning steps: 1\n",
      "    - Knowledge items: 3\n",
      "    - Has transformations: False\n",
      "\n",
      "Test example 1:\n",
      "  - Exact match: False\n",
      "  - Cell accuracy: 0.0000\n",
      "\n",
      "Task summary:\n",
      "  - Exact matches: 0/1\n",
      "  - Task accuracy: 0.0000\n",
      "  - Execution time: 28.31 seconds\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Task 3/3: 8e1813be.json\n",
      "================================================================================\n",
      "\n",
      "Initial blackboard state:\n",
      "  Blackboard summary:\n",
      "    - Reasoning steps: 0\n",
      "    - Knowledge items: 1\n",
      "    - Has transformations: False\n",
      "\n",
      "Solving with LLM module...\n",
      "Analyzing grid transformation pattern.\n",
      "\n",
      "BLACKBOARD_OUTPUT:\n",
      "```json\n",
      "```\n",
      "{\"confidence\": 0.9, \"logical_predicates\": {\"grid_size_change\": true, \"input_grid_rows\": 18, \"input_grid_cols\": 15, \"output_grid_rows\": 6, \"output_grid_cols\": 6}, \"transformations\": [], \"predictions\": [{\"test_index\": 0, \"input_grid\": [[2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 0, 5, 5, 5, 5, 5, 5, 5, 0, 3, 3, 3], [0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0], [8, 8, 8, 8, 8, 8, 8, 0, 5, 5, 5, 5, 5, 5, 5, 0, 8, 8, 8], [0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]], \"output_grid\": [[2, 2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3, 3], [8, 8, 8, 8, 8, 8, 8], [4, 4, 4, 4, 4, 4, 4], [6, 6, 6, 6, 6, 6, 6], [1, 1, 1, 1, 1, 1, 1], [7, 7, 7, 7, 7, 7, 7]], \"changes\": {\"total_changes\": \"N/A (different shapes)\", \"percentage_changed\": 0, \"changes_by_color\": {}}}]}\n",
      "```\n",
      "\n",
      "PREDICTED OUTPUTS:\n",
      "Test Example 1 Input:\n",
      "2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 2 2 2\n",
      "0 0 0 0 0 0 0 0 5 5 5 5 5 5 5 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 5 5 5 5 5 5 5 0 0 0 0\n",
      "3 3 3 3 3 3 3 0 5 5 5 5 5 5 5 0 3 3 3\n",
      "0 0 0 0 0 0 0 0 5 5 5 5 5 5 5 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 5 5 5 5 5 5 5 0 0 0 0\n",
      "8 8 8 8 8 8 8 0 5 5 5 5 5 5 5 0 8 8 8\n",
      "0 0 0 0 0 0 0 0 5 5 5 5 5 5 5 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "Error parsing blackboard data: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Blackboard state after LLM reasoning:\n",
      "  Blackboard summary:\n",
      "    - Reasoning steps: 1\n",
      "    - Knowledge items: 3\n",
      "    - Has transformations: False\n",
      "\n",
      "Test example 1:\n",
      "  - Exact match: False\n",
      "  - Cell accuracy: 0.0000\n",
      "\n",
      "Task summary:\n",
      "  - Exact matches: 0/1\n",
      "  - Task accuracy: 0.0000\n",
      "  - Execution time: 25.63 seconds\n",
      "\n",
      "ft:gpt-4o-mini-2024-07-18:personal:arc-agi-blackboard:BJoraD1a Evaluation Results:\n",
      "Overall Accuracy: 0.0000\n",
      "Exact Matches: 0/3\n",
      "Total evaluation time: 53.94 seconds\n",
      "Average time per task: 17.98 seconds\n",
      "Results saved to output/evaluation/llm/ft:gpt_4o_mini_2024_07_18:personal:arc_agi_blackboard:BJoraD1a_evaluation.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from task4 import Task\n",
    "from llm_module import LLMReasoningModule\n",
    "import traceback\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"data/training\"\n",
    "RESULTS_DIR = \"output/evaluation/llm\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Load tasks\n",
    "def load_tasks(directory, limit=None):\n",
    "    \"\"\"Load tasks from directory with optional limit\"\"\"\n",
    "    tasks = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    try:\n",
    "                        data = json.load(f)\n",
    "                        if \"train\" not in data or \"test\" not in data:\n",
    "                            print(f\"Warning: Invalid task format in {file_path}\")\n",
    "                            continue\n",
    "                        \n",
    "                        task = Task(\n",
    "                            task_id=os.path.basename(file_path),\n",
    "                            train_pairs=[(pair[\"input\"], pair[\"output\"]) for pair in data[\"train\"]],\n",
    "                            test_pairs=[(pair[\"input\"], pair[\"output\"]) for pair in data[\"test\"]],\n",
    "                        )\n",
    "                        tasks.append(task)\n",
    "                        \n",
    "                        if limit and len(tasks) >= limit:\n",
    "                            return tasks\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {file_path}: {e}\")\n",
    "    return tasks\n",
    "\n",
    "# Initialize LLM module with your fine-tuned model\n",
    "def create_llm_module(model_name=\"gpt-4\", api_key=None, temperature=0.3):\n",
    "    \"\"\"Create LLM module with specified model\"\"\"\n",
    "    # Get API key from environment if not provided\n",
    "    api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OpenAI API key not provided. Set OPENAI_API_KEY environment variable.\")\n",
    "        \n",
    "    # Create module\n",
    "    llm_module = LLMReasoningModule(\n",
    "        model=model_name,\n",
    "        api_key=api_key,\n",
    "        temperature=temperature,\n",
    "        max_tokens=2048,\n",
    "        log_path=\"logs/llm_evaluation\",\n",
    "        cache_responses=True\n",
    "    )\n",
    "    \n",
    "    return llm_module\n",
    "\n",
    "# Evaluate a single prediction\n",
    "def evaluate_prediction(prediction, target):\n",
    "    \"\"\"Check if prediction exactly matches target\"\"\"\n",
    "    prediction = np.array(prediction)\n",
    "    target = np.array(target)\n",
    "    \n",
    "    # Check if shapes match\n",
    "    if prediction.shape != target.shape:\n",
    "        return {\n",
    "            \"exact_match\": False,\n",
    "            \"shape_match\": False,\n",
    "            \"accuracy\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Shape matches, check cell-by-cell accuracy\n",
    "    total_cells = target.size\n",
    "    matching_cells = np.sum(prediction == target)\n",
    "    cell_accuracy = matching_cells / total_cells if total_cells > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": np.array_equal(prediction, target),\n",
    "        \"shape_match\": True,\n",
    "        \"accuracy\": float(cell_accuracy)\n",
    "    }\n",
    "\n",
    "# Function to examine blackboard state\n",
    "def examine_blackboard(task, prefix=\"\"):\n",
    "    \"\"\"Examine and print blackboard state\"\"\"\n",
    "    if not hasattr(task, 'blackboard'):\n",
    "        print(f\"{prefix}No blackboard found\")\n",
    "        return {}\n",
    "    \n",
    "    # Get blackboard state\n",
    "    blackboard_info = {}\n",
    "    \n",
    "    # Extract reasoning history\n",
    "    reasoning_history = task.get_reasoning_history()\n",
    "    blackboard_info[\"reasoning_steps\"] = len(reasoning_history)\n",
    "    \n",
    "    # Extract confidence scores\n",
    "    if hasattr(task.blackboard, 'confidence_scores'):\n",
    "        blackboard_info[\"confidence_scores\"] = task.blackboard.confidence_scores\n",
    "\n",
    "    # Check for transformations\n",
    "    if hasattr(task.blackboard, 'knowledge_base'):\n",
    "        # New blackboard format\n",
    "        transformations_keys = [k for k in task.blackboard.knowledge_base.keys() \n",
    "                                if 'transformation' in k]\n",
    "        blackboard_info[\"has_transformations\"] = len(transformations_keys) > 0\n",
    "        blackboard_info[\"transformation_keys\"] = transformations_keys\n",
    "        \n",
    "        # Count total knowledge items\n",
    "        blackboard_info[\"knowledge_items\"] = len(task.blackboard.knowledge_base)\n",
    "    \n",
    "    # For readable output, print summary\n",
    "    if prefix:\n",
    "        print(f\"{prefix}Blackboard summary:\")\n",
    "        print(f\"{prefix}  - Reasoning steps: {blackboard_info['reasoning_steps']}\")\n",
    "        print(f\"{prefix}  - Knowledge items: {blackboard_info.get('knowledge_items', 'N/A')}\")\n",
    "        print(f\"{prefix}  - Has transformations: {blackboard_info.get('has_transformations', 'N/A')}\")\n",
    "        \n",
    "    return blackboard_info\n",
    "\n",
    "# Detailed evaluation of LLM module\n",
    "def evaluate_llm_module(llm_module, tasks, verbose=True, save_path=None):\n",
    "    \"\"\"Evaluate LLM module on tasks\"\"\"\n",
    "    results = {\n",
    "        \"model_name\": llm_module.model,\n",
    "        \"task_results\": {},\n",
    "        \"overall_accuracy\": 0.0,\n",
    "        \"exact_matches\": 0,\n",
    "        \"total_grids\": 0,\n",
    "        \"errors\": [],\n",
    "        \"timing\": {\n",
    "            \"total_time\": 0,\n",
    "            \"avg_time_per_task\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process each task\n",
    "    for i, task in enumerate(tqdm(tasks, desc=f\"Evaluating {llm_module.model}\")):\n",
    "        if verbose:\n",
    "            print(f\"\\n\\n{'='*80}\")\n",
    "            print(f\"Task {i+1}/{len(tasks)}: {task.task_id}\")\n",
    "            print(f\"{'='*80}\")\n",
    "        \n",
    "        task_start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Examine blackboard before LLM reasoning\n",
    "            if verbose:\n",
    "                print(\"\\nInitial blackboard state:\")\n",
    "                examine_blackboard(task, prefix=\"  \")\n",
    "            \n",
    "            # Use the module's solve method\n",
    "            if verbose:\n",
    "                print(\"\\nSolving with LLM module...\")\n",
    "            \n",
    "            predictions = llm_module.solve(task)\n",
    "            \n",
    "            # Examine blackboard after LLM reasoning\n",
    "            if verbose:\n",
    "                print(\"\\nBlackboard state after LLM reasoning:\")\n",
    "                blackboard_info = examine_blackboard(task, prefix=\"  \")\n",
    "            \n",
    "            # Evaluate each prediction\n",
    "            task_exact_matches = 0\n",
    "            task_total = len(task.test_pairs)\n",
    "            prediction_results = []\n",
    "            \n",
    "            for i, (_, target_grid) in enumerate(task.test_pairs):\n",
    "                if i < len(predictions):\n",
    "                    eval_result = evaluate_prediction(predictions[i], target_grid)\n",
    "                    prediction_results.append(eval_result)\n",
    "                    task_exact_matches += int(eval_result[\"exact_match\"])\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"\\nTest example {i+1}:\")\n",
    "                        print(f\"  - Exact match: {eval_result['exact_match']}\")\n",
    "                        print(f\"  - Cell accuracy: {eval_result['accuracy']:.4f}\")\n",
    "            \n",
    "            # Calculate task accuracy\n",
    "            task_accuracy = task_exact_matches / task_total if task_total > 0 else 0.0\n",
    "            task_duration = time.time() - task_start_time\n",
    "            \n",
    "            # Store results\n",
    "            results[\"task_results\"][task.task_id] = {\n",
    "                \"exact_match_accuracy\": task_accuracy,\n",
    "                \"exact_matches\": task_exact_matches,\n",
    "                \"total\": task_total,\n",
    "                \"prediction_details\": prediction_results,\n",
    "                \"execution_time\": task_duration,\n",
    "            }\n",
    "            \n",
    "            # Update overall counts\n",
    "            results[\"exact_matches\"] += task_exact_matches\n",
    "            results[\"total_grids\"] += task_total\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nTask summary:\")\n",
    "                print(f\"  - Exact matches: {task_exact_matches}/{task_total}\")\n",
    "                print(f\"  - Task accuracy: {task_accuracy:.4f}\")\n",
    "                print(f\"  - Execution time: {task_duration:.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"task_id\": task.task_id,\n",
    "                \"error\": str(e),\n",
    "                \"traceback\": traceback.format_exc()\n",
    "            }\n",
    "            results[\"errors\"].append(error_info)\n",
    "            if verbose:\n",
    "                print(f\"\\nError evaluating task {task.task_id}: {e}\")\n",
    "                print(traceback.format_exc())\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    total_time = time.time() - start_time\n",
    "    results[\"overall_accuracy\"] = (\n",
    "        results[\"exact_matches\"] / results[\"total_grids\"] \n",
    "        if results[\"total_grids\"] > 0 else 0.0\n",
    "    )\n",
    "    results[\"timing\"][\"total_time\"] = total_time\n",
    "    results[\"timing\"][\"avg_time_per_task\"] = total_time / len(tasks) if tasks else 0\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{llm_module.model} Evaluation Results:\")\n",
    "    print(f\"Overall Accuracy: {results['overall_accuracy']:.4f}\")\n",
    "    print(f\"Exact Matches: {results['exact_matches']}/{results['total_grids']}\")\n",
    "    print(f\"Total evaluation time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per task: {results['timing']['avg_time_per_task']:.2f} seconds\")\n",
    "    \n",
    "    if results[\"errors\"]:\n",
    "        print(f\"Encountered {len(results['errors'])} errors during evaluation\")\n",
    "    \n",
    "    # Save results if requested\n",
    "    if save_path:\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Results saved to {save_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Main evaluation function\n",
    "def main(model_name=\"gpt-4\", limit_tasks=5, verbose=True):\n",
    "    print(f\"Loading up to {limit_tasks} tasks...\")\n",
    "    tasks = load_tasks(DATA_DIR, limit=limit_tasks)\n",
    "    print(f\"Loaded {len(tasks)} tasks\")\n",
    "    \n",
    "    try:\n",
    "        # Create LLM module with specified model\n",
    "        print(f\"Creating LLM module with model: {model_name}\")\n",
    "        llm_module = create_llm_module(model_name=model_name)\n",
    "        \n",
    "        # Evaluate the module\n",
    "        print(\"Evaluating LLM module...\")\n",
    "        results_path = os.path.join(RESULTS_DIR, f\"{model_name.replace('-', '_')}_evaluation.json\")\n",
    "        results = evaluate_llm_module(\n",
    "            llm_module=llm_module,\n",
    "            tasks=tasks,\n",
    "            verbose=verbose,\n",
    "            save_path=results_path\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Critical error in evaluation: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e), \"traceback\": traceback.format_exc()}\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    # You can adjust these parameters as neededwhich\n",
    "    results = main(\n",
    "        model_name=\"ft:gpt-4o-mini-2024-07-18:personal:arc-agi-blackboard:BJoraD1a\",  # Replace with your fine-tuned model name\n",
    "        limit_tasks=3,\n",
    "        verbose=True         # Set to True for detailed output\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
