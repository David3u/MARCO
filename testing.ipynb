{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from task import Task\n",
    "import trainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_static_cell(input_grid, output_grid, r, c):\n",
    "    \"\"\"\n",
    "    Determine if a cell is static (unchanged) between input and output grids.\n",
    "    A static cell has the same color and same color neighbors (NESW).\n",
    "    \n",
    "    Args:\n",
    "        input_grid: Input grid as numpy array\n",
    "        output_grid: Output grid as numpy array\n",
    "        r: Row index\n",
    "        c: Column index\n",
    "        \n",
    "    Returns:\n",
    "        Boolean indicating if the cell is static\n",
    "    \"\"\"\n",
    "    # Check if shapes match for this comparison\n",
    "    if input_grid.shape != output_grid.shape:\n",
    "        return False\n",
    "    \n",
    "    # Check if the cell value is the same\n",
    "    if input_grid[r, c] != output_grid[r, c]:\n",
    "        return False\n",
    "    \n",
    "    # Check if neighbors have the same values\n",
    "    directions = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "    for dr, dc in directions:\n",
    "        nr, nc = r + dr, c + dc\n",
    "        # Check if neighbor is within bounds\n",
    "        if 0 <= nr < input_grid.shape[0] and 0 <= nc < input_grid.shape[1]:\n",
    "            # If any neighbor changed, the cell is not static\n",
    "            if input_grid[nr, nc] != output_grid[nr, nc]:\n",
    "                return False\n",
    "    \n",
    "    # If we get here, the cell has the same value and all neighbors have the same values\n",
    "    return True\n",
    "\n",
    "def pad_grids_to_same_size(grid1, grid2, padding_value=10):\n",
    "    \"\"\"\n",
    "    Pad both grids to the same size using the specified padding value.\n",
    "    \n",
    "    Args:\n",
    "        grid1: First grid as numpy array\n",
    "        grid2: Second grid as numpy array\n",
    "        padding_value: Value to use for padding (default: 10)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (padded_grid1, padded_grid2)\n",
    "    \"\"\"\n",
    "    max_rows = max(grid1.shape[0], grid2.shape[0])\n",
    "    max_cols = max(grid1.shape[1], grid2.shape[1])\n",
    "    \n",
    "    # Create padded grids\n",
    "    padded_grid1 = np.full((max_rows, max_cols), padding_value, dtype=grid1.dtype)\n",
    "    padded_grid2 = np.full((max_rows, max_cols), padding_value, dtype=grid2.dtype)\n",
    "    \n",
    "    # Copy original grids into padded grids\n",
    "    padded_grid1[:grid1.shape[0], :grid1.shape[1]] = grid1\n",
    "    padded_grid2[:grid2.shape[0], :grid2.shape[1]] = grid2\n",
    "    \n",
    "    return padded_grid1, padded_grid2\n",
    "\n",
    "def evaluate_model(model, tasks):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on a set of tasks with explicit shape prediction metrics.\n",
    "    Ignores static cells (cells that don't change between input and output) when calculating accuracy.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        tasks: List of Task objects\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics including node-level accuracy and shape prediction metrics\n",
    "    \"\"\"\n",
    "    total_tasks = len(tasks)\n",
    "    total_train_grids = sum(len(task.train_pairs) for task in tasks)\n",
    "    total_test_grids = sum(len(task.test_pairs) for task in tasks)\n",
    "    total_grids = total_train_grids + total_test_grids\n",
    "    \n",
    "    correct_tasks = 0\n",
    "    correct_train_grids = 0\n",
    "    correct_test_grids = 0\n",
    "    \n",
    "    # Node-level accuracy tracking\n",
    "    total_train_nodes = 0\n",
    "    total_test_nodes = 0\n",
    "    correct_train_nodes = 0\n",
    "    correct_test_nodes = 0\n",
    "    \n",
    "    # Shape prediction metrics\n",
    "    total_shape_predictions = 0\n",
    "    correct_shape_predictions = 0\n",
    "    \n",
    "    # Evaluate each task\n",
    "    with tqdm(total=len(tasks), desc=\"Evaluating model on tasks\") as pbar:\n",
    "        for task in tasks:\n",
    "            # Run model to get predictions\n",
    "            predictions = model.solve(task)\n",
    "            \n",
    "            # Check predictions match\n",
    "            if len(predictions) != len(task.test_pairs):\n",
    "                print(f\"Warning: Number of predictions ({len(predictions)}) doesn't match number of test pairs ({len(task.test_pairs)}) for task {task.task_id}\")\n",
    "            \n",
    "            # Count correct train grids and nodes\n",
    "            train_correct = 0\n",
    "\n",
    "            train_predictions = model.solve(task)\n",
    "            \n",
    "            for i, ((input_grid, expected), predicted) in enumerate(zip(task.train_pairs, train_predictions)):\n",
    "                input_np = np.array(input_grid)\n",
    "                expected_np = np.array(expected)\n",
    "                \n",
    "                # Pad grids to same size if shapes don't match\n",
    "                if input_np.shape != expected_np.shape:\n",
    "                    input_padded, expected_padded = pad_grids_to_same_size(input_np, expected_np)\n",
    "                else:\n",
    "                    input_padded, expected_padded = input_np, expected_np\n",
    "                \n",
    "                # Pad prediction to match expected size\n",
    "                if predicted.shape != expected_padded.shape:\n",
    "                    predicted_padded = np.full(expected_padded.shape, 10, dtype=predicted.dtype)\n",
    "                    min_rows = min(predicted.shape[0], expected_padded.shape[0])\n",
    "                    min_cols = min(predicted.shape[1], expected_padded.shape[1])\n",
    "                    predicted_padded[:min_rows, :min_cols] = predicted[:min_rows, :min_cols]\n",
    "                else:\n",
    "                    predicted_padded = predicted\n",
    "                \n",
    "                # Calculate static cell mask\n",
    "                static_mask = np.zeros(expected_padded.shape, dtype=bool)\n",
    "                for r in range(expected_padded.shape[0]):\n",
    "                    for c in range(expected_padded.shape[1]):\n",
    "                        static_mask[r, c] = is_static_cell(input_padded, expected_padded, r, c)\n",
    "                \n",
    "                # Calculate accuracy ignoring static cells\n",
    "                non_static_mask = ~static_mask\n",
    "                \n",
    "                # For grid-level accuracy: check if all non-static cells are correct\n",
    "                if non_static_mask.sum() == 0:\n",
    "                    # If all cells are static, grid is correct if prediction matches expected\n",
    "                    is_correct = np.array_equal(predicted_padded, expected_padded)\n",
    "                else:\n",
    "                    # Check non-static cells only\n",
    "                    non_static_correct = (predicted_padded[non_static_mask] == expected_padded[non_static_mask])\n",
    "                    is_correct = non_static_correct.all()\n",
    "                \n",
    "                # For node-level accuracy: count correct non-static cells\n",
    "                if non_static_mask.sum() == 0:\n",
    "                    # If all cells are static, use traditional accuracy\n",
    "                    train_grid_total_nodes = expected_padded.size\n",
    "                    train_grid_correct_nodes = np.sum(predicted_padded == expected_padded)\n",
    "                else:\n",
    "                    # Count only non-static cells\n",
    "                    train_grid_total_nodes = non_static_mask.sum()\n",
    "                    train_grid_correct_nodes = np.sum(predicted_padded[non_static_mask] == expected_padded[non_static_mask])\n",
    "                    \n",
    "                    # Add penalty for incorrectly predicted static cells (optional, can be adjusted)\n",
    "                    static_incorrect = np.sum((predicted_padded[static_mask] != expected_padded[static_mask]))\n",
    "                    if static_incorrect > 0:\n",
    "                        # Add static incorrect cells to total count (penalty)\n",
    "                        train_grid_total_nodes += static_incorrect\n",
    "                \n",
    "                total_train_nodes += train_grid_total_nodes\n",
    "                correct_train_nodes += train_grid_correct_nodes\n",
    "                    \n",
    "                # Update grid level metrics\n",
    "                if is_correct:\n",
    "                    train_correct += 1\n",
    "                    correct_train_grids += 1\n",
    "            \n",
    "            # Count correct test grids and nodes\n",
    "            test_correct = 0\n",
    "            \n",
    "            for i, ((input_grid, expected), predicted) in enumerate(zip(task.test_pairs, predictions)):\n",
    "                input_np = np.array(input_grid)\n",
    "                expected_np = np.array(expected)\n",
    "                \n",
    "                # Track shape prediction accuracy\n",
    "                total_shape_predictions += 1\n",
    "                shape_is_correct = (predicted.shape == expected_np.shape)\n",
    "                if shape_is_correct:\n",
    "                    correct_shape_predictions += 1\n",
    "                \n",
    "                # Pad grids to same size if shapes don't match\n",
    "                if input_np.shape != expected_np.shape:\n",
    "                    input_padded, expected_padded = pad_grids_to_same_size(input_np, expected_np)\n",
    "                else:\n",
    "                    input_padded, expected_padded = input_np, expected_np\n",
    "                \n",
    "                # Pad prediction to match expected size\n",
    "                if predicted.shape != expected_padded.shape:\n",
    "                    predicted_padded = np.full(expected_padded.shape, 10, dtype=predicted.dtype)\n",
    "                    min_rows = min(predicted.shape[0], expected_padded.shape[0])\n",
    "                    min_cols = min(predicted.shape[1], expected_padded.shape[1])\n",
    "                    predicted_padded[:min_rows, :min_cols] = predicted[:min_rows, :min_cols]\n",
    "                else:\n",
    "                    predicted_padded = predicted\n",
    "                \n",
    "                # Calculate static cell mask\n",
    "                static_mask = np.zeros(expected_padded.shape, dtype=bool)\n",
    "                for r in range(expected_padded.shape[0]):\n",
    "                    for c in range(expected_padded.shape[1]):\n",
    "                        static_mask[r, c] = is_static_cell(input_padded, expected_padded, r, c)\n",
    "                \n",
    "                # Calculate accuracy ignoring static cells\n",
    "                non_static_mask = ~static_mask\n",
    "                \n",
    "                # For grid-level accuracy: check if all non-static cells are correct\n",
    "                if non_static_mask.sum() == 0:\n",
    "                    # If all cells are static, grid is correct if prediction matches expected\n",
    "                    is_correct = np.array_equal(predicted_padded, expected_padded)\n",
    "                else:\n",
    "                    # Check non-static cells only\n",
    "                    non_static_correct = (predicted_padded[non_static_mask] == expected_padded[non_static_mask])\n",
    "                    is_correct = non_static_correct.all()\n",
    "                \n",
    "                # For node-level accuracy: count correct non-static cells\n",
    "                if non_static_mask.sum() == 0:\n",
    "                    # If all cells are static, use traditional accuracy\n",
    "                    test_grid_total_nodes = expected_padded.size\n",
    "                    test_grid_correct_nodes = np.sum(predicted_padded == expected_padded)\n",
    "                else:\n",
    "                    # Count only non-static cells\n",
    "                    test_grid_total_nodes = non_static_mask.sum()\n",
    "                    test_grid_correct_nodes = np.sum(predicted_padded[non_static_mask] == expected_padded[non_static_mask])\n",
    "                    \n",
    "                    # Add penalty for incorrectly predicted static cells (optional, can be adjusted)\n",
    "                    static_incorrect = np.sum((predicted_padded[static_mask] != expected_padded[static_mask]))\n",
    "                    if static_incorrect > 0:\n",
    "                        # Add static incorrect cells to total count (penalty)\n",
    "                        test_grid_total_nodes += static_incorrect\n",
    "                \n",
    "                total_test_nodes += test_grid_total_nodes\n",
    "                correct_test_nodes += test_grid_correct_nodes\n",
    "                    \n",
    "                # Update grid level metrics\n",
    "                if is_correct:\n",
    "                    test_correct += 1\n",
    "                    correct_test_grids += 1\n",
    "            \n",
    "            # A task is correct if all its test pairs are correct\n",
    "            if test_correct == len(task.test_pairs):\n",
    "                correct_tasks += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Calculate overall grid metrics\n",
    "    correct_grids = correct_train_grids + correct_test_grids\n",
    "    train_grid_accuracy = correct_train_grids / total_train_grids if total_train_grids > 0 else 0\n",
    "    test_grid_accuracy = correct_test_grids / total_test_grids if total_test_grids > 0 else 0\n",
    "    overall_grid_accuracy = correct_grids / total_grids if total_grids > 0 else 0\n",
    "    task_accuracy = correct_tasks / total_tasks if total_tasks > 0 else 0\n",
    "    \n",
    "    # Calculate shape prediction metrics\n",
    "    shape_accuracy = correct_shape_predictions / total_shape_predictions if total_shape_predictions > 0 else 0\n",
    "    \n",
    "    # Calculate node-level metrics\n",
    "    total_nodes = total_train_nodes + total_test_nodes\n",
    "    correct_nodes = correct_train_nodes + correct_test_nodes\n",
    "    train_node_accuracy = correct_train_nodes / total_train_nodes if total_train_nodes > 0 else 0\n",
    "    test_node_accuracy = correct_test_nodes / total_test_nodes if total_test_nodes > 0 else 0\n",
    "    overall_node_accuracy = correct_nodes / total_nodes if total_nodes > 0 else 0\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nEvaluation Results (Static-Cell-Aware):\")\n",
    "    print(f\"Correct tasks: {correct_tasks}/{total_tasks} ({task_accuracy:.2%})\")\n",
    "    print(f\"Correct test grids: {correct_test_grids}/{total_test_grids} ({test_grid_accuracy:.2%})\")\n",
    "    print(f\"Shape prediction accuracy: {correct_shape_predictions}/{total_shape_predictions} ({shape_accuracy:.2%})\")\n",
    "    print(f\"Correct train grids: {correct_train_grids}/{total_train_grids} ({train_grid_accuracy:.2%})\")\n",
    "    print(f\"Correct total grids: {correct_grids}/{total_grids} ({overall_grid_accuracy:.2%})\")\n",
    "    print(f\"\\nNode-level Accuracy (ignoring static cells):\")\n",
    "    print(f\"Test nodes: {correct_test_nodes}/{total_test_nodes} ({test_node_accuracy:.2%})\")\n",
    "    print(f\"Train nodes: {correct_train_nodes}/{total_train_nodes} ({train_node_accuracy:.2%})\")\n",
    "    print(f\"Overall nodes: {correct_nodes}/{total_nodes} ({overall_node_accuracy:.2%})\")\n",
    "    print(f\"\\nNote: Static cells (unchanged between input/output with same neighbors) are ignored in accuracy calculation.\")\n",
    "    \n",
    "    return {\n",
    "        \"task_accuracy\": task_accuracy,\n",
    "        \"test_grid_accuracy\": test_grid_accuracy,\n",
    "        \"shape_accuracy\": shape_accuracy,\n",
    "        \"train_grid_accuracy\": train_grid_accuracy,\n",
    "        \"overall_grid_accuracy\": overall_grid_accuracy,\n",
    "        \"correct_tasks\": correct_tasks,\n",
    "        \"total_tasks\": total_tasks,\n",
    "        \"correct_test_grids\": correct_test_grids,\n",
    "        \"total_test_grids\": total_test_grids,\n",
    "        \"correct_shape_predictions\": correct_shape_predictions,\n",
    "        \"total_shape_predictions\": total_shape_predictions,\n",
    "        \"correct_train_grids\": correct_train_grids,\n",
    "        \"total_train_grids\": total_train_grids,\n",
    "        \"correct_grids\": correct_grids,\n",
    "        \"total_grids\": total_grids,\n",
    "        # Node-level metrics\n",
    "        \"test_node_accuracy\": test_node_accuracy,\n",
    "        \"train_node_accuracy\": train_node_accuracy,\n",
    "        \"overall_node_accuracy\": overall_node_accuracy,\n",
    "        \"correct_test_nodes\": correct_test_nodes,\n",
    "        \"total_test_nodes\": total_test_nodes,\n",
    "        \"correct_train_nodes\": correct_train_nodes,\n",
    "        \"total_train_nodes\": total_train_nodes,\n",
    "        \"correct_nodes\": correct_nodes,\n",
    "        \"total_nodes\": total_nodes\n",
    "    }\n",
    "\n",
    "def preprocess_task_graphs(tasks, padding_value=10):\n",
    "    \"\"\"\n",
    "    Preprocess all graphs in the given tasks once, instead of during each solve call.\n",
    "    \n",
    "    Args:\n",
    "        tasks: List of Task objects\n",
    "        padding_value: Padding value for standardizing dimensions\n",
    "        \n",
    "    Returns:\n",
    "        The tasks with preprocessed graphs\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    expected_dim = 3  # Standardized input dimension\n",
    "    \n",
    "    for task in tasks:\n",
    "        # Preprocess test graphs\n",
    "        for test_graph in task.test_graphs:\n",
    "            if hasattr(test_graph, 'x'):\n",
    "                # Convert from one-hot to class labels if needed\n",
    "                if test_graph.x.dim() == 2 and test_graph.x.size(1) == 11:\n",
    "                    test_graph.x = test_graph.x.argmax(dim=1)\n",
    "\n",
    "                # Ensure x is long and 2D\n",
    "                test_graph.x = test_graph.x.long()\n",
    "                if test_graph.x.dim() == 1:\n",
    "                    test_graph.x = test_graph.x.unsqueeze(1)  # Shape: (nodes, 1)\n",
    "\n",
    "                # Standardize shape to (nodes, expected_dim)\n",
    "                if test_graph.x.size(1) < expected_dim:\n",
    "                    pad = torch.full((test_graph.x.size(0), expected_dim), padding_value, dtype=torch.long)\n",
    "                    pad[:, :test_graph.x.size(1)] = test_graph.x\n",
    "                    test_graph.x = pad\n",
    "                elif test_graph.x.size(1) > expected_dim:\n",
    "                    test_graph.x = test_graph.x[:, :expected_dim]\n",
    "\n",
    "                # Extract positional info\n",
    "                test_graph.pos = test_graph.x[:, 1:3].float() if expected_dim >= 3 else None\n",
    "                test_graph.x = test_graph.x[:, 0].long().unsqueeze(1)  # Final x: shape (nodes, 1)\n",
    "                \n",
    "                # Mark as preprocessed\n",
    "                test_graph.preprocessed = True\n",
    "        \n",
    "        # If the task also has train graphs, preprocess them too\n",
    "        if hasattr(task, 'train_graphs'):\n",
    "            for train_graph in task.train_graphs:\n",
    "                if hasattr(train_graph, 'x'):\n",
    "                    # Convert from one-hot to class labels if needed\n",
    "                    if train_graph.x.dim() == 2 and train_graph.x.size(1) == 11:\n",
    "                        train_graph.x = train_graph.x.argmax(dim=1)\n",
    "    \n",
    "                    # Ensure x is long and 2D\n",
    "                    train_graph.x = train_graph.x.long()\n",
    "                    if train_graph.x.dim() == 1:\n",
    "                        train_graph.x = train_graph.x.unsqueeze(1)  # Shape: (nodes, 1)\n",
    "    \n",
    "                    # Standardize shape to (nodes, expected_dim)\n",
    "                    if train_graph.x.size(1) < expected_dim:\n",
    "                        pad = torch.full((train_graph.x.size(0), expected_dim), padding_value, dtype=torch.long)\n",
    "                        pad[:, :train_graph.x.size(1)] = train_graph.x\n",
    "                        train_graph.x = pad\n",
    "                    elif train_graph.x.size(1) > expected_dim:\n",
    "                        train_graph.x = train_graph.x[:, :expected_dim]\n",
    "    \n",
    "                    # Extract positional info\n",
    "                    train_graph.pos = train_graph.x[:, 1:3].float() if expected_dim >= 3 else None\n",
    "                    train_graph.x = train_graph.x[:, 0].long().unsqueeze(1)  # Final x: shape (nodes, 1)\n",
    "                    \n",
    "                    # Mark as preprocessed\n",
    "                    train_graph.preprocessed = True\n",
    "    \n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from unified_module import UnifiedReasoningModule\n",
    "from nlm_module import NLMReasoningModule\n",
    "import trainer\n",
    "from task import Blackboard\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model path\n",
    "UNIFIED_PATH = \"output/models/unified_shape/unified_shape_final.pt\"\n",
    "NLM_PATH = \"output/models/nlm_shape/nlm_shape_final.pt\"\n",
    "DATA_PATH = \"precomputed_tasks/evaluation_400\"\n",
    "\n",
    "results_all_runs = []\n",
    "\n",
    "# Load tasks\n",
    "tasks = trainer.load_precomputed_tasks(DATA_PATH)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nStarting evaluation run {i+1}/5\")\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Reset task states if needed\n",
    "    for task in tasks:\n",
    "        if hasattr(task, 'blackboard'):\n",
    "            task.blackboard = Blackboard()\n",
    "    \n",
    "    # Initialize model\n",
    "    # print(\"Initializing unified model...\")\n",
    "    # model = UnifiedReasoningModule(\n",
    "    #     input_dim=3,\n",
    "    #     hidden_dim=128,\n",
    "    #     output_dim=11,\n",
    "    #     device=device\n",
    "    # )\n",
    "    # model.load_complete_state(UNIFIED_PATH)\n",
    "\n",
    "    \n",
    "    # Initialize model and preprocess tasks for nlm\n",
    "    trainer.preprocess_task_graphs(tasks)\n",
    "    print(\"Initializing nlm model...\")\n",
    "    model = NLMReasoningModule(\n",
    "            input_dim=3,\n",
    "            hidden_dim=128,\n",
    "            output_dim=11,\n",
    "            device=device\n",
    "        )\n",
    "    model.load_complete_state(NLM_PATH)\n",
    "\n",
    "    # Move model to device\n",
    "    model.model = model.model.to(device)\n",
    "    model.model.eval() # Set to evaluation mode\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluate_model(model, tasks)\n",
    "    results_all_runs.append(results)\n",
    "    \n",
    "    # Delete model to ensure clean state for next iteration\n",
    "    del model\n",
    "    \n",
    "    # Clear CUDA cache again\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Analyze results across all runs\n",
    "print(\"\\nResults across all runs:\")\n",
    "for i, run_results in enumerate(results_all_runs):\n",
    "    print(f\"Run {i+1}: {run_results}\")\n",
    "\n",
    "# Calculate average performance\n",
    "if results_all_runs:\n",
    "    avg_accuracy = sum(r.get('accuracy', 0) for r in results_all_runs) / len(results_all_runs)\n",
    "    print(f\"Average accuracy: {avg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from unified_module import UnifiedReasoningModule\n",
    "from nlm_module import NLMReasoningModule\n",
    "from task_adaptation_runner import TaskAdaptationRunner\n",
    "import trainer\n",
    "from task5 import Blackboard, Task\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model paths\n",
    "UNIFIED_MAML_PATH = \"output/models/unified_shape/unified_maml_final_model.pt\"\n",
    "NLM_MAML_PATH = \"output/models/nlm_shape/nlm_maml_final_model.pt\"\n",
    "USE_BLACKBOARD_INSIGHTS = True  # Whether to use insights from blackboard\n",
    "\n",
    "def evaluate_task_adaptation(tasks, model_path, reasoning_module_class, method=\"maml\", inner_lr=0.001, inner_steps=5):\n",
    "    \"\"\"\n",
    "    Evaluate using task adaptation with the TaskAdaptationRunner\n",
    "    \n",
    "    Args:\n",
    "        tasks: List of tasks to evaluate\n",
    "        model_path: Path to the pre-trained meta-learning model\n",
    "        reasoning_module_class: Class of reasoning module (UnifiedReasoningModule or NLMReasoningModule)\n",
    "        method: Meta-learning method (\"maml\" or \"proto\")\n",
    "        inner_lr: Inner loop learning rate\n",
    "        inner_steps: Number of inner loop steps\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation results including shape metrics\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating with {method.upper()} task adaptation with ILR:{inner_lr} and IS:{inner_steps}...\")\n",
    "    \n",
    "    # Determine reasoning method string\n",
    "    reasoning_method = \"unified\" if reasoning_module_class.__name__ == \"UnifiedReasoningModule\" else \"nlm\"\n",
    "    \n",
    "    # Initialize task adaptation runner with the correct device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    runner = TaskAdaptationRunner(\n",
    "        inner_lr=inner_lr,\n",
    "        inner_steps=inner_steps,\n",
    "        model_path=model_path,\n",
    "        model_config={'input_dim': 3},\n",
    "        reasoning_module_class=reasoning_module_class,\n",
    "        method=method,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create a wrapper class to make TaskAdaptationRunner compatible with evaluate_model\n",
    "    class TaskAdaptationWrapper:\n",
    "        def __init__(self, runner):\n",
    "            self.runner = runner\n",
    "            \n",
    "        def solve(self, task):\n",
    "            return self.runner.adapt_to_task(\n",
    "                task=task,\n",
    "                visualize=False,\n",
    "                save_dir=None\n",
    "            )\n",
    "    \n",
    "    # Create the wrapper instance\n",
    "    model_wrapper = TaskAdaptationWrapper(runner)\n",
    "    \n",
    "    # Get full results from evaluate_model\n",
    "    results = evaluate_model(model_wrapper, tasks)\n",
    "    \n",
    "    # Return the metrics without redundant recalculation\n",
    "    return {\n",
    "        \"full_accuracy\": results['test_grid_accuracy'],\n",
    "        \"shape_accuracy\": results['shape_accuracy'],\n",
    "        \"content_accuracy\": results['test_node_accuracy'],\n",
    "        \"task_accuracy\": results['task_accuracy'],\n",
    "        \"test_grid_accuracy\": results['test_grid_accuracy'],\n",
    "        \"test_node_accuracy\": results['test_node_accuracy']\n",
    "    }\n",
    "\n",
    "def run_evaluation(tasks, reasoning_module_class, model_path, inner_lr, inner_steps):\n",
    "    \"\"\"Run multiple evaluation runs and return averaged results with shape metrics\"\"\"\n",
    "    num_runs = 5  # Number of evaluation runs\n",
    "    all_results = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\n=== Starting evaluation run {run+1}/{num_runs} ===\")\n",
    "        \n",
    "        # Clear CUDA cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Reset tasks' blackboards if needed\n",
    "        for task in tasks:\n",
    "            if hasattr(task, 'blackboard'):\n",
    "                task.blackboard = Blackboard()\n",
    "        \n",
    "        # Run evaluation with task adaptation\n",
    "        run_results = evaluate_task_adaptation(\n",
    "            tasks=tasks,\n",
    "            model_path=model_path,\n",
    "            reasoning_module_class=reasoning_module_class,\n",
    "            inner_lr=inner_lr,\n",
    "            inner_steps=inner_steps,\n",
    "            method=\"maml\"\n",
    "        )\n",
    "        \n",
    "        all_results.append(run_results)\n",
    "        \n",
    "        # Print comprehensive metrics for this run\n",
    "        print(f\"Run {run+1} metrics:\")\n",
    "        print(f\"  Full accuracy: {run_results.get('full_accuracy', 0.0):.4f} (shape + content)\")\n",
    "        print(f\"  Shape accuracy: {run_results.get('shape_accuracy', 0.0):.4f} (correct dimensions)\")\n",
    "        print(f\"  Content accuracy: {run_results.get('content_accuracy', 0.0):.4f} (overlapping cells)\")\n",
    "        print(f\"  Grid accuracy: {run_results.get('test_grid_accuracy', 0.0):.4f}\")\n",
    "        print(f\"  Node accuracy: {run_results.get('test_node_accuracy', 0.0):.4f}\")\n",
    "    \n",
    "    # Calculate average performance for all metrics\n",
    "    metrics = [\n",
    "        \"full_accuracy\", \"shape_accuracy\", \"content_accuracy\", \n",
    "        \"test_grid_accuracy\", \"test_node_accuracy\", \"task_accuracy\"\n",
    "    ]\n",
    "    \n",
    "    avg_metrics = {}\n",
    "    for metric in metrics:\n",
    "        # Use get() to handle cases where old metrics might be missing\n",
    "        values = [result.get(metric, 0.0) for result in all_results]\n",
    "        avg_metrics[f\"average_{metric}\"] = sum(values) / len(values) if values else 0.0\n",
    "    \n",
    "    # Print comprehensive average metrics\n",
    "    print(f\"\\nAverage metrics across {num_runs} runs:\")\n",
    "    print(f\"Full accuracy: {avg_metrics.get('average_full_accuracy', 0.0):.4f} (shape + content)\")\n",
    "    print(f\"Shape accuracy: {avg_metrics.get('average_shape_accuracy', 0.0):.4f} (correct dimensions)\")\n",
    "    print(f\"Content accuracy: {avg_metrics.get('average_content_accuracy', 0.0):.4f} (overlapping cells)\")\n",
    "    print(f\"Task accuracy: {avg_metrics.get('average_task_accuracy', 0.0):.4f}\")\n",
    "    print(f\"Grid accuracy: {avg_metrics.get('average_test_grid_accuracy', 0.0):.4f}\")\n",
    "    print(f\"Node accuracy: {avg_metrics.get('average_test_node_accuracy', 0.0):.4f}\")\n",
    "    \n",
    "    # Return comprehensive results\n",
    "    return {\n",
    "        \"all_runs\": all_results,\n",
    "        **avg_metrics  # Include all average metrics\n",
    "    }\n",
    "\n",
    "# Test the static cell evaluation\n",
    "# test_static_cell_evaluation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MODELS = [\"unified\", \"nlm\"]\n",
    "    INNER_LR = 0.05\n",
    "    INNER_STEPS = [5, 10, 15]\n",
    "    \n",
    "    for MODEL in MODELS:\n",
    "        for IS in INNER_STEPS:\n",
    "            \n",
    "            # Load evaluation tasks\n",
    "            print(\"Loading evaluation tasks...\")\n",
    "            tasks = trainer.load_precomputed_tasks(\"precomputed_tasks/evaluation_400\")\n",
    "            \n",
    "            # Run evaluation\n",
    "            if MODEL==\"nlm\":\n",
    "                tasks = trainer.preprocess_task_graphs(tasks)\n",
    "                results = run_evaluation(tasks, NLMReasoningModule, NLM_MAML_PATH, INNER_LR, IS)\n",
    "            else:\n",
    "                results = run_evaluation(tasks, UnifiedReasoningModule, UNIFIED_MAML_PATH, INNER_LR, IS)\n",
    "            \n",
    "            # Convert numpy values to Python native types for JSON serialization\n",
    "            def convert_numpy(obj):\n",
    "                if isinstance(obj, np.number):\n",
    "                    return float(obj)\n",
    "                elif isinstance(obj, np.ndarray):\n",
    "                    return obj.tolist()\n",
    "                elif isinstance(obj, dict):\n",
    "                    return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "                elif isinstance(obj, list):\n",
    "                    return [convert_numpy(i) for i in obj]\n",
    "                return obj\n",
    "            \n",
    "            with open(f\"{MODEL}_{IS}_05_task_adaptation_results.json\", \"w\") as f:\n",
    "                json.dump(convert_numpy(results), f, indent=2)\n",
    "            \n",
    "            print(\"Evaluation completed and results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LLM Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from task import Task\n",
    "from llm_module import LLMReasoningModule\n",
    "import traceback\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"data/training\"\n",
    "RESULTS_DIR = \"output/evaluation/llm\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Load tasks\n",
    "def load_tasks(directory, limit=None):\n",
    "    \"\"\"Load tasks from directory with optional limit\"\"\"\n",
    "    tasks = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    try:\n",
    "                        data = json.load(f)\n",
    "                        if \"train\" not in data or \"test\" not in data:\n",
    "                            print(f\"Warning: Invalid task format in {file_path}\")\n",
    "                            continue\n",
    "                        \n",
    "                        task = Task(\n",
    "                            task_id=os.path.basename(file_path),\n",
    "                            train_pairs=[(pair[\"input\"], pair[\"output\"]) for pair in data[\"train\"]],\n",
    "                            test_pairs=[(pair[\"input\"], pair[\"output\"]) for pair in data[\"test\"]],\n",
    "                        )\n",
    "                        tasks.append(task)\n",
    "                        \n",
    "                        if limit and len(tasks) >= limit:\n",
    "                            return tasks\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {file_path}: {e}\")\n",
    "    return tasks\n",
    "\n",
    "# Initialize LLM module with your fine-tuned model\n",
    "def create_llm_module(model_name=\"gpt-4\", api_key=None, temperature=0.3):\n",
    "    \"\"\"Create LLM module with specified model\"\"\"\n",
    "    # Get API key from environment if not provided\n",
    "    api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OpenAI API key not provided. Set OPENAI_API_KEY environment variable.\")\n",
    "        \n",
    "    # Create module\n",
    "    llm_module = LLMReasoningModule(\n",
    "        model=model_name,\n",
    "        api_key=api_key,\n",
    "        temperature=temperature,\n",
    "        max_tokens=2048,\n",
    "        log_path=\"logs/llm_evaluation\",\n",
    "        cache_responses=True\n",
    "    )\n",
    "    \n",
    "    return llm_module\n",
    "\n",
    "# Evaluate a single prediction\n",
    "def evaluate_prediction(prediction, target):\n",
    "    \"\"\"Check if prediction exactly matches target\"\"\"\n",
    "    prediction = np.array(prediction)\n",
    "    target = np.array(target)\n",
    "    \n",
    "    # Check if shapes match\n",
    "    if prediction.shape != target.shape:\n",
    "        return {\n",
    "            \"exact_match\": False,\n",
    "            \"shape_match\": False,\n",
    "            \"accuracy\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Shape matches, check cell-by-cell accuracy\n",
    "    total_cells = target.size\n",
    "    matching_cells = np.sum(prediction == target)\n",
    "    cell_accuracy = matching_cells / total_cells if total_cells > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": np.array_equal(prediction, target),\n",
    "        \"shape_match\": True,\n",
    "        \"accuracy\": float(cell_accuracy)\n",
    "    }\n",
    "\n",
    "def evaluate_prediction_static_aware(prediction, target, input_grid):\n",
    "    \"\"\"Check if prediction matches target, ignoring static cells\"\"\"\n",
    "    prediction = np.array(prediction)\n",
    "    target = np.array(target)\n",
    "    input_grid = np.array(input_grid)\n",
    "    \n",
    "    # Pad grids to same size if shapes don't match\n",
    "    if input_grid.shape != target.shape:\n",
    "        input_padded, target_padded = pad_grids_to_same_size(input_grid, target)\n",
    "    else:\n",
    "        input_padded, target_padded = input_grid, target\n",
    "    \n",
    "    # Pad prediction to match target size\n",
    "    if prediction.shape != target_padded.shape:\n",
    "        prediction_padded = np.full(target_padded.shape, 10, dtype=prediction.dtype)\n",
    "        min_rows = min(prediction.shape[0], target_padded.shape[0])\n",
    "        min_cols = min(prediction.shape[1], target_padded.shape[1])\n",
    "        prediction_padded[:min_rows, :min_cols] = prediction[:min_rows, :min_cols]\n",
    "        shape_match = False\n",
    "    else:\n",
    "        prediction_padded = prediction\n",
    "        shape_match = True\n",
    "    \n",
    "    # Calculate static cell mask\n",
    "    static_mask = np.zeros(target_padded.shape, dtype=bool)\n",
    "    for r in range(target_padded.shape[0]):\n",
    "        for c in range(target_padded.shape[1]):\n",
    "            static_mask[r, c] = is_static_cell(input_padded, target_padded, r, c)\n",
    "    \n",
    "    # Calculate accuracy ignoring static cells\n",
    "    non_static_mask = ~static_mask\n",
    "    \n",
    "    if non_static_mask.sum() == 0:\n",
    "        # If all cells are static, use traditional accuracy\n",
    "        total_cells = target_padded.size\n",
    "        matching_cells = np.sum(prediction_padded == target_padded)\n",
    "        exact_match = np.array_equal(prediction_padded, target_padded)\n",
    "    else:\n",
    "        # Count only non-static cells\n",
    "        total_cells = non_static_mask.sum()\n",
    "        matching_cells = np.sum(prediction_padded[non_static_mask] == target_padded[non_static_mask])\n",
    "        exact_match = matching_cells == total_cells\n",
    "        \n",
    "        # Add penalty for incorrectly predicted static cells\n",
    "        static_incorrect = np.sum((prediction_padded[static_mask] != target_padded[static_mask]))\n",
    "        if static_incorrect > 0:\n",
    "            total_cells += static_incorrect\n",
    "    \n",
    "    cell_accuracy = matching_cells / total_cells if total_cells > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": exact_match,\n",
    "        \"shape_match\": shape_match,\n",
    "        \"accuracy\": float(cell_accuracy),\n",
    "        \"static_cells\": static_mask.sum(),\n",
    "        \"non_static_cells\": non_static_mask.sum()\n",
    "    }\n",
    "\n",
    "def test_static_cell_evaluation():\n",
    "    \"\"\"Test the static cell evaluation functionality\"\"\"\n",
    "    print(\"Testing static cell evaluation...\")\n",
    "    \n",
    "    # Test case 1: Simple pattern completion\n",
    "    input_grid = np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 0]\n",
    "    ])\n",
    "    \n",
    "    expected_output = np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 1],  # Only (1,2) changes\n",
    "        [0, 0, 0]\n",
    "    ])\n",
    "    \n",
    "    # Perfect prediction\n",
    "    perfect_prediction = expected_output.copy()\n",
    "    \n",
    "    # Prediction with one error in static cell\n",
    "    static_error_prediction = expected_output.copy()\n",
    "    static_error_prediction[0, 0] = 1  # Error in static cell\n",
    "    \n",
    "    # Prediction with one error in non-static cell\n",
    "    dynamic_error_prediction = expected_output.copy()\n",
    "    dynamic_error_prediction[1, 2] = 0  # Error in the only changing cell\n",
    "    \n",
    "    # Test perfect prediction\n",
    "    result1 = evaluate_prediction_static_aware(perfect_prediction, expected_output, input_grid)\n",
    "    print(f\"Perfect prediction: accuracy={result1['accuracy']:.3f}, exact_match={result1['exact_match']}\")\n",
    "    print(f\"  Static cells: {result1['static_cells']}, Non-static cells: {result1['non_static_cells']}\")\n",
    "    \n",
    "    # Test static error prediction\n",
    "    result2 = evaluate_prediction_static_aware(static_error_prediction, expected_output, input_grid)\n",
    "    print(f\"Static error prediction: accuracy={result2['accuracy']:.3f}, exact_match={result2['exact_match']}\")\n",
    "    print(f\"  Static cells: {result2['static_cells']}, Non-static cells: {result2['non_static_cells']}\")\n",
    "    \n",
    "    # Test dynamic error prediction\n",
    "    result3 = evaluate_prediction_static_aware(dynamic_error_prediction, expected_output, input_grid)\n",
    "    print(f\"Dynamic error prediction: accuracy={result3['accuracy']:.3f}, exact_match={result3['exact_match']}\")\n",
    "    print(f\"  Static cells: {result3['static_cells']}, Non-static cells: {result3['non_static_cells']}\")\n",
    "    \n",
    "    print(\"\\nNote: Static cell errors add penalty to total count, dynamic cell errors directly reduce accuracy.\")\n",
    "\n",
    "# Function to examine blackboard state\n",
    "def examine_blackboard(task, prefix=\"\"):\n",
    "    \"\"\"Examine and print blackboard state\"\"\"\n",
    "    if not hasattr(task, 'blackboard'):\n",
    "        print(f\"{prefix}No blackboard found\")\n",
    "        return {}\n",
    "    \n",
    "    # Get blackboard state\n",
    "    blackboard_info = {}\n",
    "    \n",
    "    # Extract reasoning history\n",
    "    reasoning_history = task.get_reasoning_history()\n",
    "    blackboard_info[\"reasoning_steps\"] = len(reasoning_history)\n",
    "    \n",
    "    # Extract confidence scores\n",
    "    if hasattr(task.blackboard, 'confidence_scores'):\n",
    "        blackboard_info[\"confidence_scores\"] = task.blackboard.confidence_scores\n",
    "\n",
    "    # Check for transformations\n",
    "    if hasattr(task.blackboard, 'knowledge_base'):\n",
    "        # New blackboard format\n",
    "        transformations_keys = [k for k in task.blackboard.knowledge_base.keys() \n",
    "                                if 'transformation' in k]\n",
    "        blackboard_info[\"has_transformations\"] = len(transformations_keys) > 0\n",
    "        blackboard_info[\"transformation_keys\"] = transformations_keys\n",
    "        \n",
    "        # Count total knowledge items\n",
    "        blackboard_info[\"knowledge_items\"] = len(task.blackboard.knowledge_base)\n",
    "    \n",
    "    # For readable output, print summary\n",
    "    if prefix:\n",
    "        print(f\"{prefix}Blackboard summary:\")\n",
    "        print(f\"{prefix}  - Reasoning steps: {blackboard_info['reasoning_steps']}\")\n",
    "        print(f\"{prefix}  - Knowledge items: {blackboard_info.get('knowledge_items', 'N/A')}\")\n",
    "        print(f\"{prefix}  - Has transformations: {blackboard_info.get('has_transformations', 'N/A')}\")\n",
    "        \n",
    "    return blackboard_info\n",
    "\n",
    "# Detailed evaluation of LLM module\n",
    "def evaluate_llm_module(llm_module, tasks, verbose=True, save_path=None):\n",
    "    \"\"\"Evaluate LLM module on tasks\"\"\"\n",
    "    results = {\n",
    "        \"model_name\": llm_module.model,\n",
    "        \"task_results\": {},\n",
    "        \"overall_accuracy\": 0.0,\n",
    "        \"exact_matches\": 0,\n",
    "        \"total_grids\": 0,\n",
    "        \"errors\": [],\n",
    "        \"timing\": {\n",
    "            \"total_time\": 0,\n",
    "            \"avg_time_per_task\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process each task\n",
    "    for i, task in enumerate(tqdm(tasks, desc=f\"Evaluating {llm_module.model}\")):\n",
    "        if verbose:\n",
    "            print(f\"\\n\\n{'='*80}\")\n",
    "            print(f\"Task {i+1}/{len(tasks)}: {task.task_id}\")\n",
    "            print(f\"{'='*80}\")\n",
    "        \n",
    "        task_start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Examine blackboard before LLM reasoning\n",
    "            if verbose:\n",
    "                print(\"\\nInitial blackboard state:\")\n",
    "                examine_blackboard(task, prefix=\"  \")\n",
    "            \n",
    "            # Use the module's solve method\n",
    "            if verbose:\n",
    "                print(\"\\nSolving with LLM module...\")\n",
    "            \n",
    "            predictions = llm_module.solve(task)\n",
    "            \n",
    "            # Examine blackboard after LLM reasoning\n",
    "            if verbose:\n",
    "                print(\"\\nBlackboard state after LLM reasoning:\")\n",
    "                blackboard_info = examine_blackboard(task, prefix=\"  \")\n",
    "            \n",
    "            # Evaluate each prediction\n",
    "            task_exact_matches = 0\n",
    "            task_total = len(task.test_pairs)\n",
    "            prediction_results = []\n",
    "            \n",
    "            for i, (_, target_grid) in enumerate(task.test_pairs):\n",
    "                if i < len(predictions):\n",
    "                    eval_result = evaluate_prediction(predictions[i], target_grid)\n",
    "                    prediction_results.append(eval_result)\n",
    "                    task_exact_matches += int(eval_result[\"exact_match\"])\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"\\nTest example {i+1}:\")\n",
    "                        print(f\"  - Exact match: {eval_result['exact_match']}\")\n",
    "                        print(f\"  - Cell accuracy: {eval_result['accuracy']:.4f}\")\n",
    "            \n",
    "            # Calculate task accuracy\n",
    "            task_accuracy = task_exact_matches / task_total if task_total > 0 else 0.0\n",
    "            task_duration = time.time() - task_start_time\n",
    "            \n",
    "            # Store results\n",
    "            results[\"task_results\"][task.task_id] = {\n",
    "                \"exact_match_accuracy\": task_accuracy,\n",
    "                \"exact_matches\": task_exact_matches,\n",
    "                \"total\": task_total,\n",
    "                \"prediction_details\": prediction_results,\n",
    "                \"execution_time\": task_duration,\n",
    "            }\n",
    "            \n",
    "            # Update overall counts\n",
    "            results[\"exact_matches\"] += task_exact_matches\n",
    "            results[\"total_grids\"] += task_total\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nTask summary:\")\n",
    "                print(f\"  - Exact matches: {task_exact_matches}/{task_total}\")\n",
    "                print(f\"  - Task accuracy: {task_accuracy:.4f}\")\n",
    "                print(f\"  - Execution time: {task_duration:.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"task_id\": task.task_id,\n",
    "                \"error\": str(e),\n",
    "                \"traceback\": traceback.format_exc()\n",
    "            }\n",
    "            results[\"errors\"].append(error_info)\n",
    "            if verbose:\n",
    "                print(f\"\\nError evaluating task {task.task_id}: {e}\")\n",
    "                print(traceback.format_exc())\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    total_time = time.time() - start_time\n",
    "    results[\"overall_accuracy\"] = (\n",
    "        results[\"exact_matches\"] / results[\"total_grids\"] \n",
    "        if results[\"total_grids\"] > 0 else 0.0\n",
    "    )\n",
    "    results[\"timing\"][\"total_time\"] = total_time\n",
    "    results[\"timing\"][\"avg_time_per_task\"] = total_time / len(tasks) if tasks else 0\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{llm_module.model} Evaluation Results:\")\n",
    "    print(f\"Overall Accuracy: {results['overall_accuracy']:.4f}\")\n",
    "    print(f\"Exact Matches: {results['exact_matches']}/{results['total_grids']}\")\n",
    "    print(f\"Total evaluation time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per task: {results['timing']['avg_time_per_task']:.2f} seconds\")\n",
    "    \n",
    "    if results[\"errors\"]:\n",
    "        print(f\"Encountered {len(results['errors'])} errors during evaluation\")\n",
    "    \n",
    "    # Save results if requested\n",
    "    if save_path:\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Results saved to {save_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Main evaluation function\n",
    "def main(model_name=\"gpt-4\", limit_tasks=5, verbose=True):\n",
    "    print(f\"Loading up to {limit_tasks} tasks...\")\n",
    "    tasks = load_tasks(DATA_DIR, limit=limit_tasks)\n",
    "    print(f\"Loaded {len(tasks)} tasks\")\n",
    "    \n",
    "    try:\n",
    "        # Create LLM module with specified model\n",
    "        print(f\"Creating LLM module with model: {model_name}\")\n",
    "        llm_module = create_llm_module(model_name=model_name)\n",
    "        \n",
    "        # Evaluate the module\n",
    "        print(\"Evaluating LLM module...\")\n",
    "        results_path = os.path.join(RESULTS_DIR, f\"{model_name.replace('-', '_')}_evaluation.json\")\n",
    "        results = evaluate_llm_module(\n",
    "            llm_module=llm_module,\n",
    "            tasks=tasks,\n",
    "            verbose=verbose,\n",
    "            save_path=results_path\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Critical error in evaluation: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e), \"traceback\": traceback.format_exc()}\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    # You can adjust these parameters as neededwhich\n",
    "    results = main(\n",
    "        model_name=\"ft:gpt-4o-mini-2024-07-18:personal:arc-agi-blackboard:BJoraD1a\",  # Replace with your fine-tuned model name\n",
    "        limit_tasks=3,\n",
    "        verbose=True         # Set to True for detailed output\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
